\section{Evaluation}
\label{sec:eval}

\begin{itemize}
  \item SIMD, numerics, one helper function is not specified yet
  \item use reference interpreter for numerics (with version)
  \item explanation about the helper function
  \item manually write AL for the helper function
\end{itemize}

\subsection{Correctness}
Wasm official test is written in Wast\textcolor{red}{(reference of Wast)}, scripting language for testing Wasm, which mainly consists of definitions and assertions.
Definitions are Wasm module definition to run, and assertions check the possible result of \textcolor{red}{executing} a Wasm module.
There are 7 kinds of assertions: AssertMalformed, AssertInvalid, AssertUnlinkable, AssertUninstantiable, AssertTrap, AssertReturn, and AssertExhaustion.
Among them, AssertMalformed, AssertInvalid, and AssertUnlinkable are related to static semantics, so we exclude these assertions.
AssertUninstantiable and AssertTrap are assertion that \textcolor{red}{executing} the module traps.
AssertReturn is an assertion that \textcolor{red}{executing} the module returns an expected value.
One interesting assertion is the AssertExhaustion, an assertion that a module falls into call stack overflow.
Because there is no rule about call stack overflow in the Wasm DSL and current Wasm specification, the exact behavior from the specification is running forever.
Therefore, our tool cannot detect call stack overflow, so we exclude AssertExhaustion.
Long story short, we targeted 3 kinds of assertions named AssertUninstantiable, AssertReturn, and AssertTrap in Wasm official test excluding SIMD.

To evaluate the correctness of \textcolor{red}{SpecTec}, we tested the generated AL by executing the Wasm official test using our \textcolor{red}{test driver}.
The AL interpreter and generated AL pass 21,363 AssertReturn, 2354 AssertTrap, and 34 AssertUninstantiable from the official test in \textcolor{red}{???} seconds, which shows the correctness of generated AL.

\subsection{Usefulness}
To assess the effectiveness of SpecTec in preventing human errors, we conducted
an in-depth investigation into actual errors that had previously occurred in
the main branch of the repository for Wasm standard. Our objective was to
determine whether SpecTec could have prevented these errors. We classified the
investigated errors into four distinct categories.

\textbf{Type Errors}
Two fixes were attributed to type errors within the specification. These
instances included an arity mismatch when invoking the auxiliary helper
function runelem, and an omission of the field TYPE during the initialization
of the element instance in the elem.drop execution semantics. We artificially
introduced each of these errors into the DSL spec document and applied SpecTec.
Subsequently, we confirmed that SpecTec effectively identified and reported the
TypeErrors. In addition, the error message gives the information about the
exact location of the error within the specification, and the reason for the
error correctly.  This result signifies its capability to detect mistakes made
by spec writers.

\textbf{Prose Errors}
Five errors were found to occur within the prose notation. These errors
encompassed free identifier issues, the absence of an immediate value after an
instruction name, and outdated steps that introduced a new variable which was
never utilized. Our automatic prose generation process proved to be free from
these types of errors, and we verified that the corresponding prose was
generated accurately, devoid of any such issues.

\textbf{Semantics Errors}
Three errors resulted in incorrect behavior of the WebAssembly specification.
We manually introduced these errors into the DSL, and subsequently executed the
official tests against them using our indirect interpreter. In all cases, the
tests returned failures as expected, indicating that our indirect interpreter
was proficient in detecting these errors.

\textbf{Editorial Fixes}
A total of \inred{ten} editorial fixes were identified. While these fixes did
not impact the behavior of the specification, they addressed typographical
errors in LaTeX or ensured consistency in writing style. Once again, SpecTec
can effectively address these types of issues.  Additionally, the consistency
in writing style within the generated spec is guaranteed by design.

The evaluation results demonstrate that SpecTec is effective at preventing a wide
range of human errors within the WebAssembly specification.  Its capabilities
extend to detecting and addressing type errors, prose errors, semantics errors,
as well as editorial fixes. This is a strong evidence that leveraging SpecTec
can significantly enhance the robustness and reliability of the specification
writing process.

\subsection{Generality}
In this section, we present the generality of SpecTec, by applying it to five
WebAssembl proposals currently at or about to enter phase 4 at the time of
writing. These proposals include Tail Call[?], Extended Constant
Expressions[?], Typed Function References[?], Garbage Collection[?], and
Multiple Memories[?].

The process involved two main steps. First, we manually extended the
specification files for each of these proposals. Next, we used SpecTec to
automatically generate both LaTeX and Prose backends. The necessary
modifications in SpecTec were minimal, primarily involving the addition of new
custom operators for notations specific to some of proposals.

The results of our evaluation are as follows:

\textbf{Successful Description using DSL}: Each of the five proposals was
successfully described using the WebAssembly Domain Specific Language (DSL).
This demonstrates the versatility and adaptability of SpecTec in effectively
capturing the semantics of diverse Wasm features.

\textbf{Transformation into LaTeX}: The descriptions of all proposals were
successfully transformed into LaTeX format. This indicates the capability of
SpecTec to generate formal notation specifications from DSL representations,
contributing to the ease of documentation.

\textbf{Semantic Translation Accuracy in AL and Prose}: Out of the 34
instructions affected by the proposals, all but three instructions had their
semantics correctly translated into Algorithmic Language (AL), and subsequently
into prose. For the three instructions with incorrect translations, we
performed a manual correction. Notably, the required changes were minimal, with
a line diff of only nine.

\textbf{Rigorous Testing and \inred{100\%} Pass Rate}: Following the manual
fixes, we conducted comprehensive testing using the tests of proposals against
the generated prose. The pass rate achieved was an impressive 100\%. This
confirms the accuracy and reliability of the generated prose descriptions,
demonstrating their alignment with the intended behavior specified in DSL.

Overall, the evaluation results indicate that the SpecTec approach is highly
effective and generalizable. It successfully accommodates a range of Wasm
proposals, accurately capturing their semantics in both formal and prose
notations. The minimal manual intervention required further underscores the
efficiency and reliability of SpecTec. These findings affirm the potential of
SpecTec as a transformative tool, not only within the domain of current version
of WebAssembly but also in broader contexts.

Furthermore, in the course of comparing the artifacts generated by SpecTec with
the actual proposals, we identified certain discrepancies. Upon closer
examination, it became evident that these disparities arose from errors within
the official proposals themselves, encompassing both formal and prose
notations. In total, we identified and reported four such errors to the
specification writer, all of which were subsequently rectified. This discovery
underscores the capacity of SpecTec to produce more reliable artifacts in
comparison to the potentially error-prone manual process. It highlights the
potential for SpecTec to not only streamline the specification generation
process but also enhance the overall accuracy and trustworthiness of the
resulting documentation in general.

\subsection{Others}
\begin{itemize}
\item anecdotal evidence
\begin{itemize}
\item no need to write prose
\item code review: readability
\end{itemize}

\item qualitative evaluation?
\begin{itemize}
\item compactness of writing? LoC?
\item arity issues?
\end{itemize}

\item more quantitative evaluation
\begin{itemize}
\item coverage of the prose/LaTeX specification
\end{itemize}
\end{itemize}
