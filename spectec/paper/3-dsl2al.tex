\section{Transformation of Wasm-DSL into AL}\label{sec:translate}

In this section, we describe the transformation process of Wasm-DSL into AL.
Wasm-DSL can express whole standard of the WebAssembly, including not only
execution semantics but also syntax and validation rules.
Since the main goal of this paper is to generate prose from execution semantics but not syntax or validation,
we will focus on how only execution semantics of Wasm is expressed using Wasm-DSL.

\subsection{Notations}

Two kinds of \textit{definitions} are needed to express the execution semantics of Wasm:
reduction rules and auxiliary helper functions.
Here, we formally define the notations for the reduction rules and the auxiliary helper functions.

First, assume that the set of DSL expressions $E$ and boolean expressions $B \subset E$ are given.
DSL expressions are basic building building blocks of DSL, including number, identifiers,
function calls, pairs, operations between them, etc. \inred{How much detail is needed for DSL expressions?}

A reduction rule $\ruleW \in \rulesW = \configsW \times \configsW \times \premsW^\ast$ is a triplet of a
configuration \textit{lhs}, a configuration \textit{rhs}, and a finite sequences of premises \textit{prem}*.
The high-level interpretation of a reduction rule $r = (lhs, rhs, prem*)$ should be straightforward:
when the current configuration of the program matches \textit{lhs} and all \textit{prem}s are
evaluated to be true, then alter the program configuration into \textit{rhs}.

A configuration $\configW \in \configsW = E_\bot \times E$ is a tuple of an optional expression for state \textit{z},
and an expression for a finite sequence of Wasm instructions, \textit{winstrs}.
\textit{z} is a syntactic representation of the internal state of the program. Specifically for WebAssembly,
a state is a pair of the current \textit{store} and the current \textit{frame}. A naive, high-level
interpretation of \textit{winstrs} is that it represents the current \textit{stack}, which grows right.
Thus, the last element of winstrs represent the top of the stack.

A premise $p \in \premsW = B \uplus \{otherwise\}$ is either a boolean expression, or a
single `otherwise` whose high-level interpretation is "negation of all previous premises".

\textbf{Example.} Recall the semantics of `ref.is\_null` in figure~\ref{fig:dsl1}.
The first rule will be parsed into the following reduction rule:
\[r_1=(\bot, [val, \text{REF.IS\_NULL}]), (\bot, [\text{CONST I32 1}]), [val = (\text{REF.NULL rt})]\]
The second rule will be parsed into the following:
\[r_2=(\bot, [val, \text{REF.IS\_NULL}]), (\bot, [\text{CONST I32 0}]), [otherwise]\]
Note that both of lhs and rhs for both rules are omitting the state expression,
and thus we notate it by using $\bot$.

A helper function $\helperW \in \helpersW = Id \times E^\ast \times E \times \premsW^\ast$ is a quadruple of
the name, parameters \textit{params}, a return expressions \textit{ret}, and a finite sequences of premises \textit{prem}*.

\textbf{Example.} `default` is an auxiliary helper function that takes a Wasm type as an input, and yields
the default Wasm value (usually zero) of that type as an output. The following is the
DSL that represents the two cases for this function:

\inred{TODO: Pretty print this}

-------------------------

\texttt{
def \$default\_(t) = (CONST I32 0)
}

\texttt{
-- if t = I32
}

\texttt{
def \$default\_(t) = (CONST I64 0)
}

\texttt{
-- if t = I64
}

-------------------------

When the type `I32` is given, `CONST I32 0` should be returned,
and when the type `I64` is given, `CONST I64` should be returned.
These definitions will be parsed into the following:
\[h_1=\text{"default\_"}, [t], \text{(CONST I32 0)}, [t = \text{I32}]\]
\[h_2=\text{"default\_"}, [t], \text{(CONST I64 0)}, [t = \text{I64}]\]


\subsection{Transformation}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.9\textwidth}
    \includegraphics[width=\textwidth]{img/trans1}
    \caption{Overview}
    \label{fig:overview}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.9\textwidth}
    \includegraphics[width=\textwidth]{img/trans2}
    \caption{Group transformer}
    \label{fig:grouptrans}
  \end{subfigure}
  \caption{DSL->AL transformation}
  \label{fig:trans}
\end{figure}

In this section, we describe the transformation of \dsl~into \al.
Figure~\ref{fig:overview} shows the whole system of transforming DSL into an \al~program.
The input of the system is the \dsl~document,
and the output of the system is a single \al~program $\mathsf{p}$.
The input undergoes the following process of transformation.
First, the input DSL is parsed into a set of definitions: either a reduction rule or a
helper function. Then, the definitions are grouped.
A group represents the set of definitions that should be bundled and transformed
into a single algorithm together.
Reduction rules are grouped based on
their target Wasm instruction, and helper functions are grouped based on their names.
As a result, multiple groups are formed. Each group is transformed into a single algorithm,
$\mathsf{A}$. The transformed algorithms are collected into a single AL program, which is the
final output of this transformation system. This AL program then can be stringified into a
prose notation specification document, or can be coupled with the AL interpreter to form a Wasm interpreter.

Figure~\ref{fig:grouptrans} shows the detail of group transformer, a component of the Figure~\ref{fig:overview}.
The input is either a group of reduction rules \textit{r}, or a group of helper functions \textit{h}.
The output is a single algorithm.
Due to the fundamental difference between mathematical rules and algorithm,
directly transforming into an algorithm is a non-trivial task.
Therefore, the process is divided into two steps: first pre-process DSL so that
it becomes more suitable for transformation, and then transform it into an algorithm.
As a first step, a group of definitions are first pre-processed into a
different group of definitions in semantics-preserving manner,
so that the new pre-processed group of definitions satisfies certain \textit{preconditions}.
The preconditions will be helpful when generating AL algorithms.
If the input group already satisfies the precondition, then this transformation will keep the input intact.
Secondly, Then new group of definition is transformed into AL.
How the AL is generated slightly differ based on whether the group is a reduction rule group or a helper function
group, but they are both based on the following fundamental approach.
% Should I explain AL -> AL?

~

\subsubsection{\textbf{DSL->DSL pre-processing}}~

The first step of the transformation is to pre-process DSL into a one that meets certain preconditions.
There are two major pre-processing: 1. Unification and
2. Animation.

\textbf{1. Unification}

Unification is a step that makes all LHS (for reduction rule) or parameters
(for helper functions) identical within the group.
This precondition already holds for most cases, with a few exceptions.
The most notable example is the `br` instruction.
Here are two simplified reduction rules for the `br` instruction
\[
r_1 = (..., [... \text{(BR 0)} ...]),  (..., ...), []
\]
\[
r_2 = (..., [... \text{(BR l+1)} ...]),  (..., ...), []
\]
The intention here is that `BR l` instruction should be applied with different rule,
depending on whether $l$ is 0 or not.
Note that the both rules do not have any premise. The condition check is implicitly assumed to be
done when matching the current configuration with LHS.

The following is the result of unification:
\[
r_1' = (..., [... \text{(BR t)} ...]),  (..., ...), [t = 0]
\]
\[
r_2' = (..., [... \text{(BR t)} ...]),  (..., ...), [t = l + 1]
\]
The temporary variable $t$ is introduced for both rules, replacing 0 of the first rule and
$l+1$ of the second rule. In order to denote what the temporary variable originally was
in each rules, new premise is added, denoting $t$ equals 0 in the first rule, and $l+1$ in the second rule.

\inred{
  TODO: Explain algorithm/method of animation: walking two AST's simultaneously.
}


\textbf{2. Animation}

There are two problems that makes it challenging to transform premises into AL.
First, if a given premise is an equality expression, then it may work as either a simple condition or
a binding of a new variable, and this should be inferred. More formally, it is required to
correctly find out where each free variable is getting bound. Second, the order
of premises can be permutated in an arbitrary order. This is problematic for
generating algorithm, since the exact order of steps are crucial in the algorithm.

Animation is a step that addresses these two challenges. Its goal is to infer
the bound variables and the order of each premise.
After animation, each premise is tagged with what variables it is binding
such that every variables are bound exactly once, and premises are ordered in a way that
all of unbound variables in each premise is already bound by the previous premises.

Let's look at the BR example again:
\[
r_1' = (..., [... \text{(BR t)} ...]),  (..., ...), [t = 0]
\]
\[
r_2' = (..., [... \text{(BR t)} ...]),  (..., ...), [t = l + 1]
\]
In the first rule, the premise t = 0 is the equality check.
In the second rule, on the other hand, the premise $t = l+1$ is actually a binding of $l$ with a new value.
Therefore, the animation process will preserve the first rule as it is, but will change the
second rule into the following:
\[
r_2'' = (..., [... \text{(BR t)} ...]),  (..., ...), [l + 1 \leftarrow t]
\]
which clearly indicates that the new variable $l$ is a new variable.
The effect of premise ordering is not illustrated in this example, but
if there were some additional premises that contains the variable $l$, then
the animation process will make sure that the premise $(l + 1 \leftarrow t)$ will be
placed before them.

We have discovered that solving the animation problem is in fact NP-hard.
This is especially because a single premise can bind more than two variables at once, as in
$(x,y) \leftarrow p$.
We prove it by reduction from a known NP-hard problem, the \textit{exact cover} problem.

\textbf{Definition.} Partition: Given a set $X$, a collection of subsets $P \subset 2^X$ is a partition of $X$ iff
1. The union of all sets in $P$ is $X$,
2. All sets in $P$ are pairwise disjoint.

For example, $P$ = \{\{a,b\}, \{c,d\}, \{e\}\} is a partition of the set $X$ = \{a, b, c, d, e\},
since 1. the union of all sets is X, and 2. any pair of subsets are disjoint.

\textbf{Definition.} Exact cover problem: Given a set $X$ and a collection of subsets $S \subset 2^X$,
exact cover problem is a decision problem to determine if there exists a partition $P$ which is a
subcollection of S.

For example, consider a collection $S = \{\{a,b\}, \{b,c\}, \{c,d,e\}\}$.
One of it's subcollection, $\{\{a,b\}, \{c,d,e\}\} \subset S$ is a partition of $X$,
so the answer to the problem is YES.
On the other hand, given a collection $S' = \{\{a,b\}, \{b,c\}, \{c,d\}, \{d,e\}\}$,
no any subcollection of $S'$ is a partition of $X$\footnote{It can be easily
verified by the parity. The union of any pair-wise disjoint subcollection of $S'$ would have an even number
of elements, but the whole set $X$ has 5 elements.}, so the answer to the problem is
NO.

Exact cover problem is a well-known NP-complete problem and is one of Karp's 21 NP-complete problems[?].
Therefore, we can prove that animation problem is NP-hard, if we reduce Exact cover problem
into animation problem in polynomial time.

\textbf{Theorem}: Animation problem is NP-hard.

\textbf{Proof}: Let's assume we are given a exact-cover problem with a set $X$ and a
collection of its subset, $S \subset 2^X$. Let n be size of $S$.
Let $S_i = \{x_{i1}, x_{i2}, ..., x_{ij}\}$ be the i-th subset of S.
Now, consider the reduction rule $(lhs, rhs, prems)$ where
$lhs$ is $(\top, [val_n, val_{n-1}, ..., val_1])$,
$rhs$ is $(\top, [])$, and
i-th premise of $prems$ be $val_i = (x_{i1}, x_{i2}, ..., x_{ij})$.
Note that this reduction rule can be constructed in linear time.
The claim is that when this rule is animated properly, this gives a solution to the exact-cover problem.
After animation, some premises will be explicitly annotated to be a variable bindings,
and they will be ordered in front of other premises. Note that the requirement for the
bound variables is that every variable must be bound exactly once. Therefore, if we collect
all premises denoted to be bounding, then the subsets $S_i$ corresponding to each bounding premises
would form a subcollection of S, which is a partition of the set $X$.

\textbf{Example.}
Let's consider the example of $X = \{a, b, c, d, e\}$ and $S = \{\{a,b\}, \{b,c\}, \{c,d,e\}\}$ again.
Following the proof above, the exact cover problem for X and S is reduced to the animation problem for the
following reduction rule:
\[r = (\top, [val_3, val_2, val_1]), (\top, []), [val_1 = (a,b), val_2 = (b,c), val_3 = (c, d, e)]\]
If we successfully solve the animation problem, then the result should look like the following:
\[r' = (\top, [val_3, val_2, val_1]), (\top, []), [(a,b) \leftarrow val_1, (c, d, e) \leftarrow val_3, val_2 = (b,c)]\]
From the result, we can reconstruct the partition of the set $X$ by looking at first two bindings,
$\{\{a, b\}, \{c, d, e\}\}$, giving the answer to the exact cover problem.

This theorem implies that there is no trivial method for solving this problem.
One obvious solution to this kind of problem is probably using a SMT solver such as Z3, since
this problem is fundamentally a constraint problem.
However, using such a heavy tool seems to be a bit over-engineering for this rather small input.
Instead, we decided to use a simple yet effective approach.
We reduce the problem into a exact cover problem\footnote{Note that the direction is opposite with
the proof}, and then adopt the well-known effective algorithm to solve exact cover problem,
the Knuth algorithm[?].

\inred{
  TODO: Explain reduction to exact cover and Knuth algorithm to solve it
}.


~

\subsubsection{\textbf{DSL->AL transformation}}~

In this section, we explain the transformation of pre-processed DSL into an AL algorithm.
We will explain the transformation in top-down manner:
first explain the flow of top-level algorithm,
and then detail expression for each sub-algorithms.

----------------

Algorithm 1. rule\_to\_algo

Input: A group of reduction rules r* = (lhs, rhs, prems)*

Output: An algorithm $\mathsf{A}$

1. lhs <- r*[0].lhs

2. instrs <- lhs\_to\_instrs(lhs)

3. For r in r*:

--a. prems = r.prems

--b. rhs = r.rhs

--c. instrs\_prems <- prems\_to\_instrs(prems)

--d. instrs\_rhs <- rhs\_to\_instrs(rhs)

--e. instrs <- instrs ++ append\_at\_innermost(instrs\_prems, instrs\_rhs)

4. (name, params) <- extract\_name\_and\_params(lhs)

5. Return ($\mathsf{algorithm}$ name (params) {instrs})

----------------

\inred{TODO: Pretty print algorithm 1}

After the group is pre-processed, the next step is to transform the group into
an algorithm. Algorithm 1 depicts the pseudocode of the transformation of group
of reduction rules into an algorithm.  First, the lhs of this reduction rule
group is extracted on line 1, and is transformed into an AL instructions on
line 2. Due to the pre-condition, it is guaranteed that every lhs of reduction
rule are identical and any lhs from any rules of the group can be used. In the
algorithm, lhs of the first rule (r*[0]) is used. The result of this
transformed lhs will be used as an initial instruction list, and will grow
iteratively by the loop that iterates over each rule on line 3.  For each rule,
premises and rhs are extracted on line a and b respectively, and then
transformed into instructions on lines c and d respectively. These two lists of
instructions are merged to form a single list of instructions by appending the
rhs-instructions into the innermost block of prems-instructions,
and then appended to the initial instruction list on line e.  Finally, the name and
parameter of this algorithm is extracted from the lhs of the reduction rule on
line 4. At last, the name, parameters, and AL instructions will be packed and
returned as the final algorithm.

\textbf{Example.} Recall the reduction rules for `REF.IS\_NULL` again:

\[r_1=(\bot, [val, \text{REF.IS\_NULL}]), (\bot, [\text{CONST I32 1}]), [val = (\text{REF.NULL rt})]\]

\[r_2=(\bot, [val, \text{REF.IS\_NULL}]), (\bot, [\text{CONST I32 0}]), [otherwise]\]

When this group of reductions is given as an input to the Algorithm 1, the following steps are taken.
First, the common lhs $(\bot, [val, \text{REF.IS\_NULL}])$ is extracted from line 1.
Then, this lhs will be transformed into the AL instructions by line 2:
\[[\mathsf{assert} ...,~\mathsf{pop}~val]\].
Next, we look at the first reduction rule and extract its premises and rhs on line a and b:
$ [val = (\text{REF.NULL rt})], (\bot, [\text{CONST I32 1}])$.
The premise is transformed into an $\mathsf{if}$ instruction on line c:
\[[\mathsf{if}~(val = \text{REF.NULL rt})~[]]\]
and the rhs is transformed into a $\mathsf{push}$ instruction on line d:
\[[\mathsf{push}~\text{(CONST I32 1)}]\]
The $\mathsf{push}$ instruction is then appended to the then-branch of the $\mathsf{if}$ instruction on line e:
\[[\mathsf{if}~(val = \text{REF.NULL rt})~[\mathsf{push} \text{(CONST I32 1)}]]\]
and the final $\mathsf{if}$ instruction will be appended after lhs-instruction.
In similar fashion, the premises and rhs of the second rule will be translated into the following:
\[[\mathsf{if}~\text{otherwise}~[\mathsf{push} \text{(CONST I32 0)}]]\]
By concatenating all of the generated AL instructions, we get this final result:
\[[
\mathsf{assert} ...,~
\mathsf{pop}~val,~
\mathsf{if}~(val = \text{REF.NULL rt})~[\mathsf{push}~\text{(CONST I32 1)}]~[\mathsf{push}~\text{(CONST I32 0)}]
]\]
Finally, we extract the name and the parameters for the algorithm, which is obtained by looking at
the target instruction, `REF.IS\_NULL`. Here, the name of algorithm is simply same as the instruction tag,
`REF.IS\_NULL`, and since this instruction does not get any other parameter (unlike `br`), the parameter of
the algorithm is also a empty list. As a result, the triplet of name, parameters, and body instructions will form
the AL algorithm.
By stringifying this algorithm into a prose, we can get the result that resembles the prose notation in
Figure~\ref{fig:prose1} very closely.

From now on, we explain three main sub-algorithms for the algorithm 1,
which transform lhs, rhs, and premises into AL instructions.

----------------

Algorithm 2. lhs\_to\_instrs

Input: A lefthand side expression $lhs = (z, winstrs)$

Assumption: Exactly one of target\_is\_on\_top (winstrs) or target\_is\_in\_context (winstrs) holds.

Output: A list of AL instructions $instrs$

1. header <- state\_to\_instrs(z)

2. If target\_is\_on\_top (winstrs)

-- a. [v\_n v\_n-1 ... v\_1 winstr] <- winstrs

-- b. instr\_i <- $\mathsf{pop}$ v\_i (for 1 <= i <= n)

-- c. Return [instr\_1, instr\_2, ..., instr\_n]

3. Else if target\_is\_in\_context (winstrs)

-- a. [ C[v\_n v\_n-1 ... v\_1 winstr \_] ] <- winstrs

-- b. instrs\_context <- context\_to\_instrs(C)

-- c. instr\_i <- $\mathsf{pop}$ v\_i (for 1 <= i <= n)

-- d. Return append\_at\_innermost (instrs\_context, [instr\_1, instr\_2, ..., instr\_n])

----------------

Algorithm 2 depicts the detail of lhs\_to\_instrs sub-algorithm.
The transformation is based on an assumption that holds for every lhs of current Wasm reduction rules:
\textit{winstrs} is either 1) a list of several Wasm instructins with the target instruction at the top of the stack,
or 2) a single \textbf{context} with the target instruction at somwhere inside it.

The first form has the following structure:
\[
w\_instrs = v_n v_{n-1} ... v_1 winstr
\]
For most of the existing rules for current version of Wasm semantics, including the one in the figure~\ref{fig:formal1}),
lhs has this form.

The second form has the following structure:
\[
w_instrs' = C[v_n v_{n-1} ... v_1 winstr \_]
\]
Here, $C$ represents a \textit{context}. We define a context as a Wasm instructions that
can contain other wasm instructions inside it. Currently, there are only two kinds of
contexts that actually appear in the left hand side of the reduction rules of the
current version of Wasm semantics: \textit{Label} and \textit{Frame}.
The rules that have the second form are relatively rare.
In fact, there are only 2 exceptions: `br` and `return`, whose reduction rules
have the second form.

The following is the process of the algorithm.
When the input $lhs = (z, winstrs)$ is given, the algorithm first
generate the AL instructions from the state expression $z$ on line 1.
Currently all it does is generating the AL instruction that is strignified into:
`Let F be the current frame`, if $z$ is not $\bot$. Then, the different instructions are
generated based on the form of the Wasm instruction stack. First branch on line 2 is taken
when the lhs is of the first form. In this case, the list of pop instructions are generated,
which pops all of the operands for the target instruction in right-to-left order.
The operand values are extracted on line 2-a, and pop instructions are generated on line 2-b.
Finally, the concatenation of these pop instructions are returned as the result on line 2-c.
Second branch on line 3 is taken when the lhs is of the second form.
In this case, in addition to the pop instructions, some context-related AL instructions are prepended.
First, the lhs is decomposed into the outter-most context $C$ and the sequence of inner-wasm instructions on line 3-a.
Then, the context $C$ is transformed into the AL instructions on line 3-b.
The main role of these instructions is to extract the information out of the context
and bind new variables to store those information.
The exact instructions that are generated are different based on what $C$ is.
For example, if $C$ is a label, following instructions are generated:

------------------

1. Let C be the current context.

2. If C is a label:

-- a. Let n be the arity of C

-- b. Let instr* be the continuation of C

-- c. Exit current context

------------------

After that, opearnds are transformed into the pop instructions as usual on line 3-c,
and the merged sequence of of two instruction sequences, context-instructions and pop-instructions, is
returned as a final result on line 3-d.

Overall, this sub-algorithms corresponds to a preparation phase, which consumes the
inputs for the target instruction (the operands in the stack, or the context) and
bind new variables that contains the information about the consumed inputs.

\inred{
  TODO: Explain prems\_to\_instr, rhs\_to\_instr
}
